---
layout: post
title: UCI Wine Quality Data
---

As part of a homework assignment I applied a few different classification methods to the [Wine Quality Data Set](http://archive.ics.uci.edu/ml/datasets/Wine+Quality) from the [UCI machine learning repository](http://archive.ics.uci.edu/ml/index.html). This is a brief summary of what I did. All the code was written in MATLAB.

The data consists of 4898 observations of the following 11 features related to the quality score of white wine[^1]:

[^1]: Red wine is also available, but we focus on white wine, which has more observations.

1.  fixed acidity 
2.  volatile acidity 
3.  citric acid 
4.  residual sugar 
5.  chlorides 
6.  free sulfur dioxide 
7.  total sulfur dioxide 
8.  density 
9.  pH 
10. sulphates 
11. alcohol 

Each variable is numeric and the goal is to predict the quality score, which is an integer between 0 and 10. I only considered binary classifiers, so I lumped the quality score into buckets of 5 and below and above five. 

![Histogram](/img/2016-10-08-wine-data/hist_wine_quality_score.png)

To start, I treated each individual feature as a model itself. That is, using only, say, the pH to predict the wine quality. This gives a quick indication of which features we might expect to be more useful for prediction. 

Throughout this analysis I use ROC curves to compare performance and I always do 10-fold cross validation (so each ROC plot has 10 lines corresponding to the 10 test folds). As you can see below, perhaps unsurprisingly, alcohol explains a great deal of the wine quality rating.

![Features](/img/2016-10-08-wine-data/features.png)

Next, I tried running 5 algorithms using the built-in MATLAB functions:

*   Logistic regression
*   Support Vector Machines
*   Classification trees (CART)
*   Random forests
*   Boosted trees (Adaboost)

Below are the results. Random forests do the best in terms of AUC with an average of approximately 0.81 and, interestingly, CART actually performs worse than the single feature alcohol. 

![Features](/img/2016-10-08-wine-data/ROC_algos.png)

Next, I experimented with a few parameters via nested cross validation. 

One possible reason for possible reason for the poor performance of CART was the complexity of the tree (each tree had upwards of 700 nodes).  Recall that CART prunes based on the following cost metric:

$$\sum_{\text{leaves j}} \sum_{x_i \in \text{leaf j}}1_{[y_i \neq \text{leaf's class}]} + \alpha [\text{no. leaves in subtree}]$$

We can easily prune back the tree by increasing this parameter $$\alpha$$.

Another parameter I wanted to play around with was the learning rate used in Adaboost. By specifying a learning rate $$<1$$, the contribution of each classifier is shrunk by this factor (that's why it's often called a shrinkage method). 

Here are the results for both nested cross validations:

![Nested CV results](/img/2016-10-08-wine-data/roc_ncv.png)


For CART, the average AUC is about 0.70 (compared to 0.66 without nested CV). From the figure, we notice the ROC curves are more straight, suggesting the tree structure is simpler. Indeed, this is the case: of the 10 trees used for the out test set, one of them had 9 nodes and the rest had 5. The figure below shows two example trees (the one on the right is the tree with 9 nodes). This tells us a simpler tree does a better job of explaining the data (which helps explain why random forests did well). A possible explanation of this is the importance of the feature alcohol. Notice in the two example trees, the first split is on the variable $$x_{11}$$, which is alcohol. Indeed, on all 10 test trees the split is on alcohol.

![Example trees from nested CV (outer test fold)](/img/2016-10-08-wine-data/trees.png)

For the boosted trees (Adaboost), the nested cross validation resulted in an average AUC of 0.80 (compared to 0.79 without nested cross validation), so the difference is small. Yet, interestingly, the average chosen parameter was 0.35 (compared to 1 for the regular cross validation), so while there was ultimately only a small change in the average AUC, a significantly smaller parameter value was typically chosen.